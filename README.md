# denoise_bert_transformer

Borrowing stable diffusion methods to train a transformer encoder and decoder models with no-propogation techniques. Inspired by this [notebook](https://github.com/ashishbamania/Tutorials-On-Artificial-Intelligence/blob/main/Training%20Without%20Backpropagation/NoPropDT_on_MNIST.ipynb) 

For the dataset, see this (link)[https://www.kaggle.com/datasets/thedevastator/dbpedia-ontology-dataset]

For the [research paper](https://arxiv.org/abs/2503.24322) that kick started this
